\reviewer{Reviewer 2}

\begin{generalcomment}
This paper presents COFFER, a scalable and efficient software-based Trusted Execution Environment (TEE) for standard RISC-V platforms. The authors ingeniously address the core scalability bottleneck in RISC-V TEEs—the limited number of enclaves and support for fragmented memory—by introducing the innovative LPMP framework, which virtualizes the constrained PMP hardware resources. Furthermore, the proposed EModules design offers a promising solution for striking a balance between enclave autonomy and a minimal Trusted Computing Base (TCB). The work is well-motivated, with a precise problem definition, elegant design, solid engineering implementation, and a comprehensive evaluation. The performance data, particularly the support for over 2,000 concurrent enclaves and the low overhead under worst-case memory fragmentation, is highly impressive. COFFER presents significant practical value and an immediate contribution to the RISC-V confidential computing community. I support the acceptance of this paper after it incorporates the following minor revisions.
\end{generalcomment}

Sincerely thanks for the thoughtful and constructive feedback. We are grateful for the specific suggestions to further strengthen our work and address them point-by-point below.

%% ---------------------- 2.1 ------------------------------

\begin{revcomment}{Comparison with Related Work}
Regarding Comparison with Related Work (Future Work Outlook)

The paper effectively employs a feature-based comparison (Table VI) to position COFFER's contributions clearly against prior art, which is very useful. A natural next step to further solidify the performance claims—specifically, to vividly illustrate the scalability breakthrough when the number of enclaves exceeds the limits of standard PMP—would be a direct, quantitative performance comparison. For instance, plotting the aggregate throughput or latency against increasing enclave counts for both COFFER and a baseline like Keystone on the same hardware would powerfully demonstrate the transition from "architecturally infeasible" to "feasible and efficient."

I understand that conducting such a comprehensive performance benchmarking against other TEEs can be non-trivial, potentially involving significant porting and setup efforts that lie beyond the scope of a revision. Therefore, I suggest the authors briefly acknowledge this perspective in their Conclusion or Future Work section. Discussing how such a direct performance/scalability comparison would complement their feature-based analysis would valuably guide future research in the community.
\end{revcomment}

\begin{revresponse}
Thank you for this insightful suggestion regarding quantitative performance comparison. We appreciate the opportunity to clarify the nature of \work's scalability breakthrough and our evaluation methodology.

We believe there may be a slight misunderstanding regarding what aspect of performance would exceed PMP limits. The key insight is that \work's LPMP design decouples the number of concurrent enclaves from PMP hardware constraints. Each enclave maintains its own private LPMP list, which is loaded into the standard PMP configuration during context switches. Therefore, the number of concurrent enclaves does not directly impact individual enclave performance---whether running 10 or 2,000 enclaves, each enclave experiences comparable performance when scheduled. As demonstrated in Figure~6a of Section~VII-A, the overhead remains stable (within 5\%) even when scaling from a single enclave to 2,048 concurrent enclaves.

The performance challenge that \textit{does} relate to exceeding PMP capacity is \textbf{memory fragmentation within a single enclave}. When an enclave's physical memory becomes highly fragmented, the number of LPMP regions required to describe its memory layout can exceed what the limited PMP registers (typically 16 entries) can accommodate. This is where LPMP's virtualization capability becomes critical.

To quantitatively demonstrate \work's performance under this challenging scenario, we conducted worst-case memory fragmentation experiments in Section~VII-B. Specifically, we forced the allocator to allocate physically non-adjacent pages for each enclave, ensuring maximum fragmentation where nearly every page requires a separate LPMP region. Under this worst-case condition:

\begin{itemize}
\item For memory-intensive workloads (\texttt{stress-ng} vm workers), enclaves without any LPMP optimization could not complete execution in reasonable time (exceeding 1 hour vs. baseline of $<$10 seconds).
\item With instruction/data split optimization alone, performance improved significantly, demonstrating the effectiveness of separating instruction and data memory regions.
\item For compute-intensive workloads (RV8 benchmarks with large memory consumption such as \texttt{qsort}, \texttt{aes}, and \texttt{norx}), TLB-enhancement optimization provided greater benefits by leveraging TLB caching of PMP results.
\item With both optimizations enabled, COFFER achieved $\le$3\% overhead even under worst-case memory fragmentation, demonstrating near-native performance.
\end{itemize}

We agree that a direct performance comparison showing aggregate throughput across increasing enclave counts would be valuable. However, such comparison faces a fundamental challenge: existing systems like Keystone are architecturally limited by PMP hardware constraints (typically 8-16 enclaves maximum). To conduct a fair comparison beyond this threshold would require either: (1) substantial architectural modifications to Keystone to incorporate LPMP-like virtualization (essentially reimplementing our core contribution), or (2) comparing against a hypothetical extended version that doesn't currently exist. As you astutely notes, this would involve ``significant porting and setup efforts that lie beyond the scope of a revision.''

In response, we have added a discussion in the \S\ref{sec:eval-compare} of paper's Section~VII explaining that direct performance comparison at scale requires substantial architectural modifications to baseline systems, and how our feature-based comparison demonstrates the qualitative breakthrough enabled by LPMP.
\end{revresponse}

%% ---------------------- 2.2 ------------------------------

\begin{revcomment}{Multi-core Configuration Clarification}
Regarding Clarification of Multi-core Configuration (Suggestion)

The design claims support for multi-core concurrent execution. To provide clarity, I suggest that the authors briefly state the multi-core configuration used in the relevant experiments (e.g., specify how many processor cores were utilized for the concurrent enclave test in Figure 6a). This simple clarification will help readers better understand the system's behavior on real hardware.
\end{revcomment}

\begin{revresponse}
Thanks for this helpful suggestion. We have clarified the multi-core configuration used in our experiments.

All performance evaluations in Section VII were conducted on the HiFive Unmatched board, which features four cores (SiFive U74 cores) and 16GB of DRAM. As stated in Section IV-A, \work enclaves support multi-threading and can concurrently execute on all system cores. For the concurrent enclave test in Figure 6a (number scalability), we launched up to 2,048 concurrent enclaves across all four cores of the HiFive Unmatched board. 

In response, we have added the clarification \S\ref{sec:eval-scale} to paper's Section VII-A.
\end{revresponse}

%% ---------------------- 2.3 ------------------------------

\begin{revcomment}{EModules Internal Security Model}
Regarding Clarification of the EModules Internal Security Model (Suggestion)

The EModules design is a significant contribution. To precisely define the enclave's TCB and prevent reader confusion, I recommend that the authors explicitly state the isolation relationship between different EModules (e.g., whether they reside in the same protection domain) in the Security Analysis (§VI) or Design (§IV-C) sections. Clarifying this key design choice will greatly enhance the paper's rigor and clarity.
\end{revcomment}

\begin{revresponse}
Thank you for this important suggestion to clarify the EModules internal security model. This clarification will indeed help readers better understand the enclave TCB composition.

To explicitly address the isolation relationship between different EModules: all EModules within a single enclave reside in the same protection domain. Specifically, all EModules execute in Supervisor Mode (S-Mode) and share the same enclave memory space, which is isolated from the host OS and other enclaves. The EApp runs in User Mode (U-Mode) within the same enclave, while the Security Monitor runs in Machine Mode (M-Mode).

Importantly, while EModules within an enclave share the same S-Mode protection domain, EModules are not shared between different enclaves. As stated in Section VI-A: ``\work enclaves do not have shared resources, including EModules, physical memory, TLB entries, and page tables.'' Each enclave has its own private instances of EModules loaded according to its permission table, ensuring isolation between enclaves.

The rationale for this design is that EModules provide trusted OS services to the EApp within the enclave. Sharing the same protection domain enables efficient function calls between EModules and reduces context switch overhead, while the permission-based dynamic loading mechanism allows users to customize the TCB by including only necessary EModules.

Just like reviewer 1's comment \ref{comment:tcb-1}, we have added the clarification \S\ref{sec:TCB} to Section VI.
\end{revresponse}

%% ---------------------- 2.4 ------------------------------

\begin{revcomment}{Side-Channel Discussion Refinement} \label{comment:tlb-sidechannel-refine}
Regarding Refinement of the Side-Channel Discussion (Suggestion)

The threat model appropriately excludes microarchitectural side-channel attacks requiring hardware modifications, and the paper insightfully notes that its design can mitigate TLB and page-table-based channels. To make the logic more rigorous, I suggest the authors briefly clarify the scope of side-channels discussed, explicitly stating that the channels excluded in §III and those discussed in §VI.A represent different subsets, with the latter being a "derivative advantage" of the architecture.
\end{revcomment}

\begin{revresponse}
Thank you for this insightful observation. We agree that clarifying this distinction will strengthen the logic of our security discussion. We have refined the side-channel discussion \S\ref{sec:tlb-side-channel} in paper's Section VI-A to clarify the scope.
\end{revresponse}

\begin{concludingresponse}[]
Thank you for your valuable comments and detailed questions on our manuscript. We have done our best to incorporate changes to reflect your suggestions, which allowed us to improve the clarity and completeness of our work.
\end{concludingresponse}
