\section{Revision Summary}

\indent \indent
Based on the reviewers' insightful comments, we improve the paper in four major aspects: xx, xx, xx, and xx.

\subsection{Scalable and Efficient Memory Isolation}

\textbf{Symmetric host/enclave treatment.}\label{sec:symmetric} In practice, the enclave memory pool is reserved at boot time, while actual allocations from this pool are fully dynamic at runtime. When enclaves are created, they request memory from the pool through the \aclu{MM}, which updates an ownership table to track which memory chunks belong to which enclave. When enclaves terminate, their memory is returned to the pool for reuse. The flexible boundary ensures that if the host OS experiences memory pressure, it can allocate from the edge of the reserved enclave pool; if that boundary region is occupied by enclaves, the \aclu{MM} migrates enclave memory to other regions within the pool to make boundary memory available to the host OS.

\noindent\textbf{Virtualizing PMP entries.}\label{sec:lpmp-replacement} The \aclu{LC} maintains the LPMP list in a most recently used order: when a LPMP entry is accessed, it is moved to the head of the list and loaded into a hardware PMP register. Entries at the tail of the list, those accessed least recently, are implicitly evicted from hardware PMP registers when new entries are loaded. This effectively implements an \aclu{LRU} replacement policy for what gets evicted, leveraging temporal locality in enclave memory access patterns. Furthermore, we have evaluated some other replacement policies, and the results \textbf{TODO: Figure Ref} shows that the MRU policy gets the best overall performance.

\noindent\textbf{TLB-enhanced LPMP.}\label{sec:tlb-enhanced-lpmp}
The theoretical scalability of LPMP depends on the active working set of memory regions rather than total enclave memory size. When the number of active memory segments exceeds available PMP entries ($N_{seg}^{active} > N_{PMP}^{max}$), trap frequency increases proportionally. However, the instruction/data split optimization reserves dedicated PMP entries for instruction memory, significantly reducing traps for code with high spatial locality. TLB-enhancement further extends effective PMP capacity by caching results for 2MB regions, enabling \work to maintain near-native performance even for large memory footprints with reasonable access locality. 
% ToDo: Make sure the content here is correct
The selective TLB invalidation strategy minimizes flush overhead by preserving cached PMP results for non-conflicting memory regions, making LPMP practical even under memory-intensive workloads.

\subsection{TCB} \label{sec:TCB}

All \acp{EModule} within a single enclave execute in the same S-mode protection domain and are not shared across enclaves; each enclave loads its own private instances according to its permission table.

\noindent \textbf{EModule ecosystem security.}
Beyond signature verification, \work supports practical lifecycle hardening for \acp{EModule}. The platform owner controls signing keys and can adopt stronger key management. Attestation mechanisms include module measurements, enabling relying parties to verify the exact module set and versions. Minimal module sizes facilitate auditing; permission tables let enclaves include only necessary modules, reducing exposure. These mechanisms help contain supply-chain risks and enable patching and policy-based control as the ecosystem matures.

\noindent \textbf{Securityâ€“performance tradeoffs.}
\work trades trap-and-emulate overhead for fine-grained memory isolation. The instruction/data split and optional TLB-enhancement recover most of the lost performance while preserving isolation via exclusive ownership and freshness principles. The autonomous design increases enclave-side TCB (e.g., \texttt{EMod\_Manager}) to reduce dependence on an untrusted OS, eliminating Iago and controlled-channel attack surfaces.

\subsection{Evaluation}

\noindent \textbf{Scalability.} \label{sec:eval-scale}
We conduct all performance evaluations on the HiFive Unmatched board with four cores SiFive U74 and 16GB DRAM. For the number scalability test (Figure 6a), we launch up to 2,048 concurrent enclaves distributed across all four cores, each running the \texttt{sha512} benchmark from the RV8 suite.

\noindent \textbf{Comparison.} \label{sec:eval-compare}
A direct quantitative performance comparison for workloads with highly fragmented memory would require substantial modifications to baseline systems and is highly duplicated to our works. Existing software-based TEEs like Keystone cannot support enclaves with such fragmented memory layouts. Our feature-based comparison and worst-case fragmentation experiments in Section~VII-B demonstrate that \work is the only evaluated system capable of executing workloads under extreme memory fragmentation (achieving $\le$3\% overhead), representing a qualitative breakthrough from ``architecturally infeasible'' to ``feasible and efficient.''

\subsection{Protection against Different Attacks}
\noindent \textbf{Side-channel attacks.} \label{sec:tlb-side-channel}
While Section III excludes side-channel attacks from our threat model in general, we note that \work's architectural design provides defense against certain side-channel subsets as a derivative advantage. Previous works have demonstrated various side-channel attacks on commodity RISC-V devices. Defending against cache-based side-channel attacks requires hardware modification and remains orthogonal to our study. However, \work provides significant mitigation against TLB-based and page-table-based side-channel attacks as a natural consequence of its design: each enclave manages page tables independently, and the SM enforces the principle of exclusive ownership of TLB through full TLB flushes on domain switches.

\noindent \textbf{I/O security.} \label{sec:io-security}
For MMIO peripheral devices, \work can add special LPMP entries for the host/enclaves to protect the I/O operations. However, there are also peripheral devices with DMA. Preventing attacks from such peripheral devices requires additional specialized hardware such as IOMMU or IOPMP. IOPMP is still under active development stage, and several RISC-V vendors have announced plans to implement it in future SoCs. IOMMU has already been ratified in non-ISA specifications, but currently only a few commodity RISC-V SoCs support it. While complete DMA protection requires these specialized hardwares, \work provides several interim mitigation strategies. \work can be deployed on platforms where DMA-capable devices are limited or can be controlled through administrative policies, reducing the DMA attack surface. \work's flexible boundary design ensures that enclave memory pools are allocated from specific physical memory regions that can be administratively configured to avoid certain DMA-capable peripherals. It is non-trivial to retrofit these features into \work without significant architectural changes. \work is designed with forward compatibility for emerging RISC-V security extensions. 